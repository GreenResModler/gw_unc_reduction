{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e5d11a-eb4c-4cde-9736-9dcd59ad22e7",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\"><b>Individual inversion: only hydraulic head data </b></h1>\n",
    "\n",
    "In this notebook, an individual inversion of hydraulic head data is implementd. In this case, we do not need to import gravchaw. However, since we can control data involved in the inversion process and the pupose is to compare outputs of different cases, we configure all necessary settings, as in the other inversion notebooks; however, only hydraulic head data are assimilated in this case.\n",
    "\n",
    "This notebook depends on outputs from the gw_flopy notebook, which must be run first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200da6be-2f71-48f3-ac4f-933677622925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import gravchaw\n",
    "import pyemu\n",
    "import flopy\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd # this function from pyemu notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d8010-c497-4a4a-97e1-5f44cebe12c4",
   "metadata": {},
   "source": [
    "Asuumed that the prerequisite groundwater notebook has already been executed, this cell creates a temporary folder to gather the generated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5569302a-f503-4dc4-9bcd-23163e6333bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy created groundwater model\n",
    "\n",
    "org_d = os.path.join('..', '01_create_gw', 'paper_gw_model')\n",
    "tmp_d = os.path.join('joint_case1')\n",
    "\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)\n",
    "shutil.copytree(org_d,tmp_d)\n",
    "\n",
    "hbd.prep_bins(tmp_d) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc70e33-bae2-4d41-82d7-2c72865ac460",
   "metadata": {},
   "source": [
    "This cell gathers observation data and gravity station coordinates files. \n",
    "In this example, both TLG data are generated from the true model, defined in \"gw_flopy\", with 5% Gaussian noise is added to simulate measurment errors. The required structure for observations is discussed later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364d04a0-da00-4131-b1e8-7d63d07a4752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joint_case1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add observation files\n",
    "\n",
    "obs_path = os.path.join('..', 'observation_data')\n",
    "\n",
    "shutil.copytree(obs_path, tmp_d, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12697ba-6063-416b-bb94-66d0784a85b0",
   "metadata": {},
   "source": [
    "This cell loads the previously created groundwater model. It serves only as a check to ensure the model runs correctly and is used here solely to extract zones or spatial limits, which is needed later in this notebook to add parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6551d92b-bfed-4608-934f-e4f9c5754874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading simulation...\n",
      "  loading simulation name file...\n",
      "  loading tdis package...\n",
      "  loading model gwf6...\n",
      "    loading package dis...\n",
      "    loading package ic...\n",
      "    loading package npf...\n",
      "    loading package sto...\n",
      "    loading package rch...\n",
      "    loading package wel...\n",
      "    loading package ghb...\n",
      "    loading package sfr...\n",
      "    loading package oc...\n",
      "    loading package obs...\n",
      "  loading solution package downsc_freyberg...\n"
     ]
    }
   ],
   "source": [
    "# load simulation\n",
    "\n",
    "sim = flopy.mf6.MFSimulation.load(sim_ws=tmp_d)\n",
    "\n",
    "gwf = sim.get_model()\n",
    "dis = gwf.dis\n",
    "ib = gwf.dis.idomain.array[0] # access the idomain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133024b5-fa8f-4658-8beb-e9ba4a5d18a2",
   "metadata": {},
   "source": [
    "Now that all required external files have been added to the temporary folder, we can begin defining the necessary settings to implement the optimization algorithm.\n",
    "\n",
    "The first step is to instantiate the \"PstFrom\" object, a pyemu's Class, which constructs the PEST++ interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aca923c-e18e-4efe-a8ad-ebd753eed38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate PstFrom \n",
    "\n",
    "template_ws = os.path.join(f'{tmp_d}_template')\n",
    "start_datetime=\"1-1-2008\"\n",
    "\n",
    "pf = pyemu.utils.PstFrom(original_d=tmp_d,   # the folder where the created model is stored\n",
    "                            new_d=template_ws,  # the pyemu working folder (\"the PEST template folder\")\n",
    "                            remove_existing=True,  # \"ensures a clean start\"\n",
    "                            longnames=True,  # \"set False if using PEST/PEST_HP\"\n",
    "                            zero_based=False,  # \"does the MODEL use zero based indices\"\n",
    "                            start_datetime=start_datetime,  # \"required when specifying temporal correlation between parameters\"\n",
    "                            echo=False)  # \"to stop PstFrom from writting lots of infromation to the notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc347-8219-43e6-8d60-03920ffff22f",
   "metadata": {},
   "source": [
    "After constructing PEST++ interface, the next step is instantiate \"ModelBridge\" object, an interface for preparing and managing inputs for our framework's coupled model.\n",
    "\n",
    "**Note that \"PstFrom\" object must be defined first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89215ba-5427-4e63-ae09-20448292b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate ModelBridge \n",
    "\n",
    "mb = gravchaw.model_bridge.ModelBridge(working_direct=template_ws,  # the template folder created using \"PstFrom\" object\n",
    "                                         pstfrom_obj=pf)  # the object storing \"PstFrom\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0da972-1953-4687-a5d0-efb0849c6f93",
   "metadata": {},
   "source": [
    "Now, we add required inputs for the ccoupled model. Let's start with gravity station coordinates. \n",
    "\n",
    "**Note that station coordinates must be in meter and stored in an ASCII file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2e623f-db28-42e1-a73f-84370d94a4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grav_output_name': 'grav.csv',\n",
       " 'station_name': ['G000',\n",
       "  'G001',\n",
       "  'G002',\n",
       "  'G003',\n",
       "  'G004',\n",
       "  'G005',\n",
       "  'G006',\n",
       "  'G007',\n",
       "  'G008',\n",
       "  'G009'],\n",
       " 'x_station': [728.4138455687,\n",
       "  880.5629516927,\n",
       "  710.3008567444,\n",
       "  572.6421416798,\n",
       "  79.9688456592,\n",
       "  108.9496277781,\n",
       "  224.8727562535,\n",
       "  275.5891249615,\n",
       "  833.4691807495,\n",
       "  626.9811081527],\n",
       " 'y_station': [1924.0703508486,\n",
       "  1782.7890380192,\n",
       "  1427.7744570632,\n",
       "  877.1395968049,\n",
       "  1474.8682280063,\n",
       "  1119.8536470503,\n",
       "  732.2356862106,\n",
       "  670.651524208,\n",
       "  228.6945968954,\n",
       "  127.2618594794],\n",
       " 'z_station': [66.0313210424,\n",
       "  64.6444932239,\n",
       "  62.7723216961,\n",
       "  64.8550356475,\n",
       "  90.2815125659,\n",
       "  86.9680833282,\n",
       "  72.1098325986,\n",
       "  69.5567769695,\n",
       "  59.6112512619,\n",
       "  55.0933101839]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add gravity station coordinates to ModelBridge\n",
    "\n",
    "mb.add_gravCoor(grav_obs_filename='grav.csv',  # the filename for TLG observations\n",
    "                        coor_filename='coord_gravstn.csv',  # the filename for TLG station coordinates\n",
    "                        index_col='station',  # the index column to be used as header in the model output. Station names are recommended.\n",
    "                        x_col='x(m)',  # the column containing station x coordinates\n",
    "                        y_col='y(m)',  # the column containing station y coordinates\n",
    "                        z_col='z(m)')  # # the column containing station z coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2aaff-2b66-4347-88c3-bfaa34b3b832",
   "metadata": {},
   "source": [
    "TLG is a measure of change between two times, so we need to add simulation time indices. **Note that time indices are zero-based, consistent with the FloPy convention. These values refer to simulation steps used to compute hydraulic heads.** \n",
    "\n",
    "In this example, time step 0 is set as one of the required times (the reference), and each subsequent time step is treated as a target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9af89ae-4432-4e68-930e-d03a3c50e31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add TLG simulation times to ModelBridge\n",
    "\n",
    "reference_time = [0]\n",
    "target_time = list(range(1,25))\n",
    "\n",
    "mb.add_gravTim(reference_time,  # time step index to extract reference heads\n",
    "               target_time,   # time step index to extract target heads\n",
    "               pair_type='fixed_side') # to to specify whether time indeices are fixed or flexible sides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc4a96-24fc-4848-9583-9be3d5e20de4",
   "metadata": {},
   "source": [
    "Corresponding elements form time index pairs to model TLG!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bac7fa-132d-4fb4-8925-bddab5d71c43",
   "metadata": {},
   "source": [
    "Up to this point, all external and internal inputs required for the coupled model have been prepared. in the next copuple cells, we define inputs to the optimization process. First we add observations to the created \"PstFrom\" object. Let's discuss some critical points regarding observations file:\n",
    "\n",
    "**1. Observation files must be in the \"template_ws\" folder, as we did earlier in the \"add observation files\" cell.**\n",
    "\n",
    "**2. Observations must be ASCII files.**\n",
    "\n",
    "**3. As consistency with model output files are required, observation files are expected to have a header in the first row and an index in the first column. See \"head.csv\" and \"grav.csv\" in the \"observation_data\" folder.**\n",
    "\n",
    "As we discuss in \"weighting observations\", all observations can be incorporated at this stage, and their influence on the inversion process controlled through data weighting.\n",
    "\n",
    "**Note that excpect for observations assimilated in the inversion, we must also add observations corresponding to variables reserved for predictions in the current step.** Through these notebooks, our predictins are hydraulic heads. We can add all hydraulic head observations. In this example, since we are performing an individual inversion of head data, all TLG data weights are set to zero later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2fd7ec8-16ca-4f17-ab69-81362118917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add observation(s) to PstFrom object\n",
    "\n",
    "# Hydraulic head observations\n",
    "df = pd.read_csv(os.path.join(template_ws,\"heads.csv\"),index_col=0)\n",
    "df.head()\n",
    "\n",
    "hds_df = pf.add_observations(\"heads.csv\",  # the filename of the observation type\n",
    "                            insfile=\"heads.csv.ins\",  # the filename of the instruction file (optional). Note that the instruction file is created by pyemu\n",
    "                            index_cols=\"time\",  # column label or column number (zero-based) to use as index\n",
    "                            use_cols=list(df.columns.values), # columns including observation values\n",
    "                            prefix=\"hds\")  # \"prefix to all observation names\"\n",
    "\n",
    "# TLG observations\n",
    "df2 = pd.read_csv(os.path.join(template_ws,\"grav.csv\"),index_col=0)\n",
    "df2.head()\n",
    "hds_df2 = pf.add_observations(\"grav.csv\", # the filename of the observation type\n",
    "                            insfile=\"grav.csv.ins\", # the filename of the instruction file (optional). Note that the instruction file is created by pyemu\n",
    "                            index_cols=\"time\", # column label or column number (zero-based) to use as index\n",
    "                            use_cols=list(df2.columns.values), # columns including observation values\n",
    "                            prefix=\"grv\") # \"prefix to all observation names\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd422c8-f84c-4f59-92db-5490be0f84c6",
   "metadata": {},
   "source": [
    "We now define adjustable paramter(s). In this example, hydraulic conductivity and specific yield are the adjustable parameters. We first specify their initial values, and then add them to the \"PstFrom\" object.\n",
    "\n",
    "**Note that files containing adjustable paramters must be in the the \"template_ws\" folder as ascii files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecfa4def-dfae-4059-be10-9ac0d1cc39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adjustable parametrs(s) to PstFrom object\n",
    "\n",
    "# hydraulic conductivity\n",
    "fhk = 'downsc_freyberg.npf_k.txt'\n",
    "\n",
    "dirc_k = os.path.join(template_ws, fhk) \n",
    "h_k = np.loadtxt(dirc_k, dtype=float)  # read the parameter file\n",
    "\n",
    "ini_val = 5.0  # intial value for hydraulic conductivity\n",
    "h_k[h_k==8.0]= ini_val\n",
    "np.savetxt(dirc_k, h_k, fmt='%.8f') # save initial values as an external file\n",
    "\n",
    "df_cst = pf.add_parameters(fhk,  # the filename of the adjustable paramter\n",
    "                    zone_array=ib,  # to exclude inactive cells\n",
    "                    par_type=\"constant\",  # type of paramterization (our case is constant)\n",
    "                    par_name_base=fhk.split('.')[1].replace(\"_\",\"\")+\"cn\",  # specify a name for the paramter\n",
    "                    pargp=fhk.split('.')[1].replace(\"_\",\"\")+\"cn\",  # specify a name for the paramter group\n",
    "                    lower_bound=5.0E-2,  # lower bound of the parameter\n",
    "                    upper_bound=5.0E+2,  # upper bound of the parameter\n",
    "                    par_style='d')  # parameter style (\"m\"/\"mult\"/\"multiplier\", \"a\"/\"add\"/\"addend\", or \"d\"/\"direct\")\n",
    "\n",
    "# specific yield\n",
    "fsy = 'downsc_freyberg.sto_sy.txt'\n",
    "\n",
    "dirc_sy = os.path.join(template_ws, fsy)\n",
    "sy = np.loadtxt(dirc_sy, dtype=float)  # read the parameter file\n",
    "\n",
    "ini_val = 0.005 # initial value for specific yield\n",
    "sy[sy==0.25]= ini_val\n",
    "np.savetxt(dirc_sy, sy, fmt='%.8f') # save initial values as an external file\n",
    "\n",
    "df_cst = pf.add_parameters(fsy,  # the filename of the adjustable paramter\n",
    "                    zone_array=ib,  # to exclude inactive cells\n",
    "                    par_type=\"constant\",  # type of paramterization (our case is constant)\n",
    "                    par_name_base=fsy.split('.')[1].replace(\"_\",\"\")+\"cn\",  # specify a name for the paramter\n",
    "                    pargp=fsy.split('.')[1].replace(\"_\",\"\")+\"cn\",  # specify a name for the paramter group\n",
    "                    lower_bound=1.0E-3,  # lower bound of the parameter\n",
    "                    upper_bound=1.0,  # upper bound of the parameter\n",
    "                    par_style='d')  # parameter style (\"m\"/\"mult\"/\"multiplier\", \"a\"/\"add\"/\"addend\", or \"d\"/\"direct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360dcb60-74fc-4e2c-b1d9-60261ddfb7d4",
   "metadata": {},
   "source": [
    "\"PstFrom\" object must access to the coupled model. In this cell, \"ModelBridge\" object manages it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d24386-08ac-4196-bc63-7fe007d872b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading simulation...\n",
      "  loading simulation name file...\n",
      "  loading tdis package...\n",
      "  loading model gwf6...\n",
      "    loading package dis...\n",
      "    loading package ic...\n",
      "    loading package npf...\n",
      "    loading package sto...\n",
      "    loading package rch...\n",
      "    loading package wel...\n",
      "    loading package ghb...\n",
      "    loading package sfr...\n",
      "    loading package oc...\n",
      "    loading package obs...\n",
      "  loading solution package downsc_freyberg...\n"
     ]
    }
   ],
   "source": [
    "# add the coupled model to Modelbridge\n",
    "\n",
    "mb.add_coupmodel_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a6726-b1cd-4d6b-8832-c5602bf46ebb",
   "metadata": {},
   "source": [
    "After adding the model, we build the pst object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e20640d-bb37-40aa-8256-dd61e5da2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noptmax:0, npar_adj:2, nnz_obs:565\n"
     ]
    }
   ],
   "source": [
    "# build the pst object\n",
    "\n",
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c9d04-b2bd-44ca-9352-8491abe9d7cd",
   "metadata": {},
   "source": [
    "We see the number of iterations (noptmax), adjustable parameters (npar_adj), and observations (nnz_obs). In the following cells we update the number of iterations and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f31b5932-aa4a-411e-8129-4cb06e7d0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of iterations to be used in the optimization process\n",
    "\n",
    "itr = 10   \n",
    "pst.control_data.noptmax = itr  # add the number of iteration to the pst file. By default iterations is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a555a-2438-432f-b45c-10e2bd7c79ff",
   "metadata": {},
   "source": [
    "By adding the observation files and build the pst file, observations are weighted 1, by default, and assimilated in the optimization process. In this cell by updating the weight, we decide which data to be assimilated.\n",
    "\n",
    "\n",
    "**Note that observations that are reserved for predictions (forcast) must be weighted zero.** In this case, TLG data are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af47754-e9df-442b-9149-795388b98f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight observations\n",
    "\n",
    "obs = pst.observation_data\n",
    "\n",
    "obs.loc[:, 'weight'] = 0\n",
    "  \n",
    "obs.loc[(obs.oname==\"hds\") & (obs['time'].str.split('-').str[0].astype(float)<=4018.5), \"weight\"] = 20  \n",
    "obs.loc[(obs.oname==\"hds\") & (obs['time'].str.split('-').str[0].astype(float)==3652.5), \"weight\"] = 0   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef0835-143d-44e9-a611-4385906ea740",
   "metadata": {},
   "source": [
    "Here we define state variable(s) to make predictions. Hydraulic heads at three points at the end of the simulation (day 4383.5) are set as forcast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d763840b-deac-40c3-9d5d-bf20aeee2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define state variable(s) to make predictions\n",
    "\n",
    "forecasts = ['oname:hds_otype:lst_usecol:trgw-0-15-16_time:4383.5',\n",
    "              'oname:hds_otype:lst_usecol:trgw-0-2-15_time:4383.5',\n",
    "              'oname:hds_otype:lst_usecol:trgw-0-2-9_time:4383.5']\n",
    "\n",
    "pst.pestpp_options['forecasts'] = forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e315bda-c03b-45ab-b941-933eea294ec5",
   "metadata": {},
   "source": [
    "Update the pst object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54f7afd2-7906-4d88-91a4-a79635cad8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noptmax:10, npar_adj:2, nnz_obs:156\n"
     ]
    }
   ],
   "source": [
    "# write the pst file\n",
    "\n",
    "pst.write(os.path.join(template_ws, f'{tmp_d}.pst'), version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef47ea0-bda7-44cc-90ff-f69c515b9d46",
   "metadata": {},
   "source": [
    "The final step involves running the optimization algorithm. Since parameter files are not automatically updated with the estimated values upon termination, our framework performs a post-optimization step. In this example, we do not perform post-optimization update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72262421-441d-4d12-9b24-0a7fd5ff80bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pestpp-glm.exe joint_case1.pst\n"
     ]
    }
   ],
   "source": [
    "# run the optimization algorithm and post-optimization parameter update\n",
    "\n",
    "out= gravchaw.execute_workflow.run(f'pestpp-glm {tmp_d}.pst', # optimization algorithm\n",
    "                                   template_ws,  # the pyemu working folder\n",
    "                                   verbose=False,  # controls whether command-line execution details (`cmd_str`) are printed to stdout.\n",
    "                                   post_updtpar=False)  # to enable the post-optimization procees. Default is `True`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618d4a1-04e0-4bd9-8cef-95f42ab6b609",
   "metadata": {},
   "source": [
    "**Output**\n",
    "\n",
    "All optimization files and outputs are stored in the \"template_ws\" folder. Outputs include:\n",
    "\n",
    "1. The estimated parameters are stored in \".par\" files, which in our case is \"joint_case3.par\".\n",
    "\n",
    "2. Parameter and prediction uncertainties are saved in ASCII files, which in our case are \"joint_case1.par.usum.csv\" and \"joint_case1.pred.usum.csv\", respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
